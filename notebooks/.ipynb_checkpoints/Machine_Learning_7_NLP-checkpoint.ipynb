{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Natural Language Processing : Classic to Deep Methods for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-Of-Word and TF-IDF:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/\n",
    "\n",
    "Recurrent Neural Networks (RNNs):\n",
    "\n",
    "https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9\n",
    "\n",
    "Long Short Term Memory networks (LSTMs):\n",
    "\n",
    "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "Word embeddings:\n",
    "\n",
    "http://jalammar.github.io/illustrated-word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/fernando.arroyo@Digital-\n",
      "[nltk_data]     Grenoble.local/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/fernando.arroyo@Digital-\n",
      "[nltk_data]     Grenoble.local/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#TOFILL\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we are going to tackle the sentiment analysis problem, a *text classification* problem. The idea is pretty simple : we want to automatically predict whether a text expresses positive or negative sentiments. To do so we will use the IMDB dataset, that contains 50000 movie reviews from the www.imdb.com website, and their corresponding sentiment : positive or negative. It is thus a binary classification problem, where we want to predict a binary target $y \\in \\{0,1\\}$. We will go through different ways of encoding a text in a vectorial form $x \\in \\mathbb{R}^d$, as well as different classification models, from classic ways to modern deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and explore a bit the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load and print the dataset\n",
    "imdb_dataset_original=pd.read_csv('../../data/IMDB Dataset.csv')\n",
    "imdb_dataset = imdb_dataset_original.copy()\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n"
     ]
    }
   ],
   "source": [
    "#Print first review:\n",
    "print(imdb_dataset[\"review\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the two classes size\n",
    "imdb_dataset['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the text is quite messy, and before encoding our text into features, we are going to go through different preprocessing steps in order to clean it:\n",
    "* Removing the HTML tags.\n",
    "* Removing other special characters : this means all non alphanumeric characters, including punctuation.\n",
    "* Lowercase the text.\n",
    "* Tokenization : split a text as a list of words now called tokens.\n",
    "* Stemming : removing all the suffixes from conjugation, plural, ... In order to bring a word back to its root form. For example.\n",
    "* Removing stopwords : words like 'to', 'a', 'the', ... are called stopwords, we remove them as they are too frequent words and generally just add noise.\n",
    "\n",
    "Fill the following functions to perform each of these steps. You are free to use the libraries of your choice to do so. Try to not reinvent the wheel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANR = re.compile('<.*?>') \n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"\n",
    "    Input: str : A string to clean from html tags\n",
    "    Output: str : The same string with html tags removed\n",
    "    \"\"\"\n",
    "    #TOFILL\n",
    "    cleantext = re.sub(CLEANR, '', text)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. The filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. The filming tec...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(remove_html_tags)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_SPE = r'['+string.punctuation+']'\n",
    "\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    \"\"\"\n",
    "    Input: str : A string to clean from non alphanumeric characters\n",
    "    Output: str : The same strings without non alphanumeric characters\n",
    "    \"\"\"\n",
    "    #TOFILL\n",
    "    cleantext = re.sub(CHAR_SPE, '', text)\n",
    "    return cleantext\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production The filming tech...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically theres a family where a little boy J...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Matteis Love in the Time of Money is a ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my alltime favorite movie a story of ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing fresh  innovative ide...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production The filming tech...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically theres a family where a little boy J...  negative\n",
       "4  Petter Matteis Love in the Time of Money is a ...  positive\n",
       "5  Probably my alltime favorite movie a story of ...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing fresh  innovative ide...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(remove_special_characters)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_text(text):\n",
    "    \"\"\"\n",
    "    Input: str : A string to lowercase\n",
    "    Output: str : The same string lowercased\n",
    "    \"\"\"\n",
    "    #TOFILL\n",
    "    lower_text = str.lower(text)\n",
    "    return lower_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production the filming tech...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically theres a family where a little boy j...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love in the time of money is a ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>probably my alltime favorite movie a story of ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>this show was an amazing fresh  innovative ide...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>if you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production the filming tech...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically theres a family where a little boy j...  negative\n",
       "4  petter matteis love in the time of money is a ...  positive\n",
       "5  probably my alltime favorite movie a story of ...  positive\n",
       "6  i sure would like to see a resurrection of a u...  positive\n",
       "7  this show was an amazing fresh  innovative ide...  negative\n",
       "8  encouraged by the positive comments about this...  negative\n",
       "9  if you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(lowercase_text)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(text):\n",
    "    \"\"\"\n",
    "    Input: str : A string to tokenize\n",
    "    Output: list of str : A list of the tokens splitted from the input string\n",
    "    \"\"\"\n",
    "    #TOFILL\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[one, of, the, other, reviewers, has, mentione...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[a, wonderful, little, production, the, filmin...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[i, thought, this, was, a, wonderful, way, to,...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[basically, theres, a, family, where, a, littl...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[petter, matteis, love, in, the, time, of, mon...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[probably, my, alltime, favorite, movie, a, st...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[i, sure, would, like, to, see, a, resurrectio...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[this, show, was, an, amazing, fresh, innovati...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[encouraged, by, the, positive, comments, abou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[if, you, like, original, gut, wrenching, laug...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  [one, of, the, other, reviewers, has, mentione...  positive\n",
       "1  [a, wonderful, little, production, the, filmin...  positive\n",
       "2  [i, thought, this, was, a, wonderful, way, to,...  positive\n",
       "3  [basically, theres, a, family, where, a, littl...  negative\n",
       "4  [petter, matteis, love, in, the, time, of, mon...  positive\n",
       "5  [probably, my, alltime, favorite, movie, a, st...  positive\n",
       "6  [i, sure, would, like, to, see, a, resurrectio...  positive\n",
       "7  [this, show, was, an, amazing, fresh, innovati...  negative\n",
       "8  [encouraged, by, the, positive, comments, abou...  negative\n",
       "9  [if, you, like, original, gut, wrenching, laug...  positive"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(tokenize_words)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words())\n",
    "\n",
    "def remove_stopwords(token_list):\n",
    "    \"\"\"\n",
    "    Input: list of str : A list of tokens\n",
    "    Output: list of str : The new list with removed stopwords tokens\n",
    "    \"\"\"\n",
    "    #TOFILL\n",
    "    no_stopwords = [word for word in token_list if not word in stopwords]\n",
    "    return no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[reviewers, mentioned, watching, 1, oz, episod...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[wonderful, production, filming, technique, un...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[wonderful, spend, time, hot, summer, weekend,...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[basically, family, boy, jake, thinks, zombie,...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[petter, matteis, love, time, money, visually,...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[probably, alltime, favorite, movie, story, se...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[resurrection, dated, seahunt, series, tech, t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[show, amazing, fresh, innovative, idea, 70s, ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[encouraged, positive, comments, film, forward...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[original, gut, wrenching, laughter, movie, yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  [reviewers, mentioned, watching, 1, oz, episod...  positive\n",
       "1  [wonderful, production, filming, technique, un...  positive\n",
       "2  [wonderful, spend, time, hot, summer, weekend,...  positive\n",
       "3  [basically, family, boy, jake, thinks, zombie,...  negative\n",
       "4  [petter, matteis, love, time, money, visually,...  positive\n",
       "5  [probably, alltime, favorite, movie, story, se...  positive\n",
       "6  [resurrection, dated, seahunt, series, tech, t...  positive\n",
       "7  [show, amazing, fresh, innovative, idea, 70s, ...  negative\n",
       "8  [encouraged, positive, comments, film, forward...  negative\n",
       "9  [original, gut, wrenching, laughter, movie, yo...  positive"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(remove_stopwords)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(token_list):\n",
    "    \"\"\"\n",
    "    Input: list of str : A list of tokens to stem\n",
    "    Output: list of str : The list of stemmed tokens\n",
    "    \"\"\"\n",
    "    #TOFILL\n",
    "    stemmed = [stemmer.stem(word) for word in token_list]\n",
    "    return stemmed \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[review, mention, watch, 1, oz, episod, youll,...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[wonder, product, film, techniqu, unassum, old...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[wonder, spend, time, hot, summer, weekend, si...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[basic, famili, boy, jake, think, zombi, close...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[petter, mattei, love, time, money, visual, st...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[probabl, alltim, favorit, movi, stori, selfle...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[resurrect, date, seahunt, seri, tech, today, ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[show, amaz, fresh, innov, idea, 70, air, 7, 8...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[encourag, posit, comment, film, forward, watc...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[origin, gut, wrench, laughter, movi, young, l...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  [review, mention, watch, 1, oz, episod, youll,...  positive\n",
       "1  [wonder, product, film, techniqu, unassum, old...  positive\n",
       "2  [wonder, spend, time, hot, summer, weekend, si...  positive\n",
       "3  [basic, famili, boy, jake, think, zombi, close...  negative\n",
       "4  [petter, mattei, love, time, money, visual, st...  positive\n",
       "5  [probabl, alltim, favorit, movi, stori, selfle...  positive\n",
       "6  [resurrect, date, seahunt, seri, tech, today, ...  positive\n",
       "7  [show, amaz, fresh, innov, idea, 70, air, 7, 8...  negative\n",
       "8  [encourag, posit, comment, film, forward, watc...  negative\n",
       "9  [origin, gut, wrench, laughter, movi, young, l...  positive"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(stem_words)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join all that together and apply it to our dataset. The following function simply chains all the preprocessing steps you just implemented. \n",
    "\n",
    "It adds the `list_output` flag, if False it will reconcatenate all the preprocessed tokens into a single string (with spaces between tokens), if True it will keep each sentence as a list of tokens. Depending on the libraries you will use for the next steps, it can be useful to have one or the other representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text_dataset(dataset, text_col_name = 'review', html_tags = True,\n",
    "                           special_chars = True, lowercase = True , stemming = True , \n",
    "                           stopwords = True, list_output = False ):\n",
    "    \"\"\"\n",
    "    Apply the choosen preprocessing steps to a corpus of texts and return the \n",
    "    preprocessed corpus. The list_output flag allows to return either a list\n",
    "    of token, or a rejoined string with spaces between the preprocessed tokens.\n",
    "    \"\"\"\n",
    "    def rejoin_text(token_list):\n",
    "        return ' '.join(token_list)\n",
    "    \n",
    "    \n",
    "    output = dataset.copy()\n",
    "    \n",
    "    if html_tags : \n",
    "        output[text_col_name] = output[text_col_name].apply(remove_html_tags)\n",
    "        \n",
    "    if special_chars :\n",
    "        output[text_col_name] = output[text_col_name].apply(remove_special_characters)\n",
    "        \n",
    "    if lowercase :\n",
    "        output[text_col_name] = output[text_col_name].apply(lowercase_text)\n",
    "    \n",
    "    #Tokenization for next steps:\n",
    "    output[text_col_name] = output[text_col_name].apply(tokenize_words)\n",
    "    \n",
    "    if stopwords :\n",
    "        output[text_col_name] = output[text_col_name].apply(remove_stopwords)\n",
    "        \n",
    "    if stemming :\n",
    "        output[text_col_name] = output[text_col_name].apply(stem_words)\n",
    "        \n",
    "    if not list_output :\n",
    "        output[text_col_name] = output[text_col_name].apply(rejoin_text)\n",
    "        \n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_clean_dataset = normalize_text_dataset(imdb_dataset_original, html_tags = True,\n",
    "                           special_chars = True, lowercase = True , stemming = True , \n",
    "                           stopwords = True, list_output = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification with Bag-Of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have cleaned the reviews of our dataset, how do we represent them as vectors in order to classify it ? \n",
    "One classic way to achieve that is the Bag-Of-Words (BOW) approach. To encode a text in a bag of word, we first need to know all the different words $w$ that appear in all our reviews, called the vocabulary : $w \\in \\mathcal{V}$. For each word $w$ we attribute an index $idx(w) = i$ with $i \\in \\{0, |\\mathcal{V}|-1\\}$, and represent a review $r$ as a vector of the size of the vocabulary $x_r \\in \\mathbb{R}^{|\\mathcal{V}|}$. To encode a review we are simply going to count how many time each word appears and assign it at its corresponding index in the bag-of-words vector : $x_{r,i} = count(w,r)$, where i = idx(w). \n",
    "\n",
    "This means that we completely disregard the words order, and simply take into account the number of times each word appears in each review to represent them. There are many variations of this concept, TF-IDF (term frequency-inverse document frequency) for example, gives more weight to uncommon words. Read more about BOW and TF-IDF there:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/\n",
    "\n",
    "Let's start with bag-of-words. In general we don't consider the whole vocabulary but only some of the most frequent words in order to reduce the dimensionality and avoid noise from rare words. Here we will only consider the 25000 most frequent words of the training set, meaning the words that are only in the test set will be ignored. Thus we have : $x_r \\in \\mathbb{R^{25000}}$.\n",
    "\n",
    "Encode all the reviews as bag-of-words, and train and evaluate a logistic regression model on the following train test splits. As we have seen previously, if we wanted to investigate this model we should also grid search for hyperparameters by doing a cross-validation with validation sets, etc. However this is not the goal today, so we'll simply go for a train/test split for this experiment. Concerning the evaluation metrics, in this case we care equally about correctly predicting the positives and the negatives, and we have a balanced dataset, thus we can simply use accuracy this time.\n",
    "\n",
    "Once again, don't do everything from scratch and try to find libraries that propose implementations of these concepts !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 25000 \n",
    "\n",
    "#Train/test split:\n",
    "\n",
    "lb=LabelBinarizer()\n",
    "sentiment_labels=lb.fit_transform(imdb_clean_dataset['sentiment'])\n",
    "\n",
    "train_reviews = imdb_clean_dataset.review[:45000]\n",
    "test_reviews = imdb_clean_dataset.review[45000:]\n",
    "\n",
    "train_sentiments = sentiment_labels[:45000]\n",
    "test_sentiments = sentiment_labels[45000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=max_vocab_size)\n",
    "train_bow = vectorizer.fit_transform(train_reviews)\n",
    "test_bow = vectorizer.transform(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<45000x25000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3185401 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x25000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 356309 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fernando.arroyo@Digital-Grenoble.local/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_train_bow = pd.DataFrame(train_bow.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fernando.arroyo@Digital-Grenoble.local/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/fernando.arroyo@Digital-Grenoble.local/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8714"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 123\n",
    "log_reg = LogisticRegression(random_state=seed)\n",
    "log_reg_fitted = log_reg.fit(train_bow, train_sentiments)\n",
    "\n",
    "log_reg_fitted.score(test_bow, test_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get about 88% accuracy, pretty good for such a simple model. Now let's do the same with a tf-idf encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL\n",
    "vectorizer2 = TfidfVectorizer(max_features=max_vocab_size)\n",
    "train_bow2 = vectorizer2.fit_transform(train_reviews)\n",
    "test_bow2 = vectorizer2.transform(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fernando.arroyo@Digital-Grenoble.local/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8834"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg2 = LogisticRegression(random_state=seed)\n",
    "log_reg2_fitted = log_reg2.fit(train_bow2, train_sentiments)\n",
    "log_reg2_fitted.score(test_bow2, test_sentiments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you should get about 90% accuracy this time. Other classic but more sophisticated features include N-grams, part-of-speech tagging and syntax trees, you can read more about these there:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/07/part-of-speechpos-tagging-dependency-parsing-and-constituency-parsing-in-nlp/\n",
    "\n",
    "But we will stop there for the classic approaches and go to deep learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...unless you are ahead of time, in this case learn about Bags of N-grams by yourself, and try them out :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fernando.arroyo@Digital-Grenoble.local/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/fernando.arroyo@Digital-Grenoble.local/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8714"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TOFILL (Optional)\n",
    "\n",
    "# sentence = imdb_clean_dataset['review'][1]\n",
    "\n",
    "# print(sentence)\n",
    "\n",
    "# ngram_object = textblob.TextBlob(sentence)\n",
    "\n",
    "# ngrams = ngram_object.ngrams(n=2)\n",
    "\n",
    "# print(ngrams)\n",
    "\n",
    "# ngrams_ = ngrams(train_bow2, 2)\n",
    "\n",
    "vectorizer_grams = CountVectorizer(max_features=max_vocab_size, ngram_range=(1, 2))\n",
    "train_grams = vectorizer_grams.fit_transform(train_reviews)\n",
    "test_grams = vectorizer_grams.transform(test_reviews)\n",
    "\n",
    "\n",
    "log_reg3 = LogisticRegression(random_state=seed)\n",
    "log_reg3_fitted = log_reg3.fit(train_grams, train_sentiments)\n",
    "log_reg3_fitted.score(test_grams, test_sentiments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks : Long Short Term Memory networks (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already covered feed-forward neural networks during the computer vision and the recommended system module. For natural language processing, one type of popular deep learning architecture is called Reccurent Neural Networks (RNNs). RNNs differ from feed-forward networks in the sense that some of their inner layers are recursively updated while iterating over the sequence of words given in input. We are going to use one specific RNN architecture called Long-Short Term Memory networks (LSTMs), which have been especially successful in various NLP tasks, including automatic translation, question answering, ... and text classification, our case study in this module.\n",
    "\n",
    "Learn more about how RNNs and LSTMs encode texts as vectors first:\n",
    "\n",
    "https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9\n",
    "\n",
    "If you want to understand in depth how one LSTM cell is working, you can go through these two articles:\n",
    "\n",
    "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using sequential models such as LSTMs, stopwords, punctuation and words suffixes carry semantics, and have thus much more importance than when using BOW-based models. Hence with these models we will only remove html tags, and keep all these :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_deep_clean_dataset = normalize_text_dataset(imdb_dataset_original, html_tags = True,\n",
    "                           special_chars = False, lowercase = True, stemming = False, \n",
    "                           stopwords = False, list_output = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_deep_clean_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping a validation set for early stopping is a good habit when training deep models, so let's resplit the dataset, and save the splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_deep_clean = imdb_deep_clean_dataset.iloc[:40000]\n",
    "valid_deep_clean = imdb_deep_clean_dataset.iloc[40000:45000]\n",
    "test_deep_clean = imdb_deep_clean_dataset.iloc[45000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '../../data/imdb_clean/'\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_deep_clean.to_csv(outdir + 'train.csv', index = False)\n",
    "valid_deep_clean.to_csv(outdir + 'valid.csv', index = False)\n",
    "test_deep_clean.to_csv(outdir + 'test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can restart the code from there in case of a crash :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '../../data/imdb_clean/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_deep_clean = pd.read_csv(outdir + 'train.csv')\n",
    "valid_deep_clean = pd.read_csv(outdir + 'valid.csv')\n",
    "test_deep_clean = pd.read_csv(outdir + 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing LSTMs with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 09:21:11.793123: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 09:21:31.551886: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-10-12 09:21:31.553951: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-10-12 09:21:31.623756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:61:00.0 name: NVIDIA GeForce GTX 1660 computeCapability: 7.5\n",
      "coreClock: 1.785GHz coreCount: 22 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 178.86GiB/s\n",
      "2022-10-12 09:21:31.623855: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-10-12 09:21:31.805370: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-10-12 09:21:31.805606: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-10-12 09:21:31.938362: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-10-12 09:21:31.965385: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-10-12 09:21:32.182585: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-10-12 09:21:32.221253: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-10-12 09:21:32.662242: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-10-12 09:21:32.663351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "#Some lines that allow for faster training with this version of tensorflow for these models\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.initializers import Constant\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Keras to implement an LSTM network. First we need to encode all the reviews as a list of indexes, where each word is replaced by its embedding index using keras \"Tokenizer\". To make all training reviews the same size, we will make them the same size as the longest review and add the special token < pad > as many time as necessary to all the other ones with the function `pad_sequences`. This token will be ignored by the LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 25000 \n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size, split=' ', oov_token='<unw>', filters=' ')\n",
    "tokenizer.fit_on_texts(pd.concat([train_deep_clean,valid_deep_clean]).review)\n",
    "\n",
    "# This encodes our sentence as a sequence of integer\n",
    "# each integer being the index of each word in the vocabulary\n",
    "train_seqs = tokenizer.texts_to_sequences(train_deep_clean.review)\n",
    "valid_seqs = tokenizer.texts_to_sequences(valid_deep_clean.review)\n",
    "test_seqs = tokenizer.texts_to_sequences(test_deep_clean.review)\n",
    "\n",
    "# We need to pad the sequences so that they are all the same length :\n",
    "# the length of the longest one\n",
    "max_seq_length = max( [len(seq) for seq in train_seqs + valid_seqs] )\n",
    "\n",
    "X_train = pad_sequences(train_seqs, max_seq_length)\n",
    "X_valid = pad_sequences(valid_seqs, max_seq_length)\n",
    "X_test = pad_sequences(test_seqs, max_seq_length)\n",
    "\n",
    "y_train = pd.get_dummies(train_deep_clean.sentiment).values[:,1]\n",
    "y_valid = pd.get_dummies(valid_deep_clean.sentiment).values[:,1]\n",
    "y_test = pd.get_dummies(test_deep_clean.sentiment).values[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fill the following function to implement a simple LSTM model : one embedding layer, one LSTM layer, and a final dense layer that yields a single score with a sigmoid activation function. Use Keras' Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model(vocab_size, embedding_dim, seq_length, lstm_out_dim):\n",
    "    #TOFILL\n",
    "        \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length))\n",
    "    \n",
    "    model.add(LSTM(units=lstm_out_dim))\n",
    "    \n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    #TOKEEP\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='SGD',metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 2730, 100)         2500000   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 2,741,001\n",
      "Trainable params: 2,741,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200  #Bigger than embedding dim, as it combines all the words of each review\n",
    "\n",
    "model = get_lstm_model(max_vocab_size, embedding_dim, max_seq_length, lstm_out_dim)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-11 13:41:01.707860: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-10-11 13:41:01.771292: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 1700000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-11 13:41:04.215942: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-10-11 13:41:05.808276: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 196s 297ms/step - loss: 0.6932 - accuracy: 0.4959 - val_loss: 0.6933 - val_accuracy: 0.4922\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 182s 292ms/step - loss: 0.6931 - accuracy: 0.5060 - val_loss: 0.6929 - val_accuracy: 0.5092\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 2\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 49.54%\n"
     ]
    }
   ],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty low accuracy isn't it ? Actually it is very easy to incorrectly train a deep neural net. Change the optimizer with \"adam\" instead of \"SGD\", add a dropout layer after the LSTM layer for regularization, and use early stopping :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropout, early stopping, adam\n",
    "def get_lstm_model_2(vocab_size, embedding_dim, seq_length, lstm_out_dim, dropout_rate):\n",
    "    #TOFILL\n",
    "            \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length))\n",
    "    \n",
    "    model.add(LSTM(units=lstm_out_dim))\n",
    "\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    #TOKEEP\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 2730, 100)         2500000   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 2,741,001\n",
      "Trainable params: 2,741,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = get_lstm_model_2(max_vocab_size, embedding_dim, max_seq_length, lstm_out_dim, dropout_rate)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL\n",
    "early_stopping = EarlyStopping(monitor=\"val_accuracy\", patience=2, verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 172s 272ms/step - loss: 0.5678 - accuracy: 0.6884 - val_loss: 0.3574 - val_accuracy: 0.8494\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 171s 273ms/step - loss: 0.2863 - accuracy: 0.8881 - val_loss: 0.4187 - val_accuracy: 0.8582\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 171s 273ms/step - loss: 0.1980 - accuracy: 0.9294 - val_loss: 0.3194 - val_accuracy: 0.8898\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 171s 274ms/step - loss: 0.1267 - accuracy: 0.9565 - val_loss: 0.3127 - val_accuracy: 0.8838\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 169s 270ms/step - loss: 0.0903 - accuracy: 0.9705 - val_loss: 0.4763 - val_accuracy: 0.8724\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 89.38%\n"
     ]
    }
   ],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. If we'd run for a longer time, we'd get a bit better results from our classic methods, but that's still quite slow for little improvement. We could also grid search for all hyper parameters (embedding and layer sizes, dropout rate, ...), but that's not the goal today, remember however that grid-search is standard when optimizing a model predictive performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sentiment for arbitrary sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can try predict the sentiment of any kind of sentence in english, try your own. You first need to encode each review as a sequence of indexes (called tokens in keras), to pad these sequances, and finally predict the score with your trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb_deep_clean_dataset = normalize_text_dataset(imdb_dataset_original, html_tags = True,\n",
    "#                            special_chars = False, lowercase = True, stemming = False, \n",
    "#                            stopwords = False, list_output = False )\n",
    "\n",
    "\n",
    "# tokenizer.fit_on_texts(pd.concat([train_deep_clean,valid_deep_clean]).review)\n",
    "\n",
    "# # This encodes our sentence as a sequence of integer\n",
    "# # each integer being the index of each word in the vocabulary\n",
    "# train_seqs = tokenizer.texts_to_sequences(train_deep_clean.review)\n",
    "# valid_seqs = tokenizer.texts_to_sequences(valid_deep_clean.review)\n",
    "# test_seqs = tokenizer.texts_to_sequences(test_deep_clean.review)\n",
    "\n",
    "# # We need to pad the sequences so that they are all the same length :\n",
    "# # the length of the longest one\n",
    "# max_seq_length = max( [len(seq) for seq in train_seqs + valid_seqs] )\n",
    "\n",
    "# X_train = pad_sequences(train_seqs, max_seq_length)\n",
    "# X_valid = pad_sequences(valid_seqs, max_seq_length)\n",
    "# X_test = pad_sequences(test_seqs, max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.84054464]]\n",
      "[[0.00472129]]\n"
     ]
    }
   ],
   "source": [
    "good = \"i really liked the movie and had fun\"\n",
    "bad = \"worst movie on the planet, so boring\"\n",
    "for review in [good,bad]:\n",
    "    \n",
    "    #TOFILL\n",
    "    token = tokenizer.texts_to_sequences([review])\n",
    "    padded = pad_sequences(token, max_seq_length)\n",
    "    pred = model.predict(padded)\n",
    "    print(pred)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize embeddings with pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of LSTMs is a bit heavy, one way to speed this up is to re-use pre-trained word embeddings. Many such embeddings are available on the net. Read this to understand how are produced word embeddings and why they encode information that helps with all NLP tasks:\n",
    "\n",
    "http://jalammar.github.io/illustrated-word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use GloVe embeddings, download and load the embeddings produced from 6 billions documents from : https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../../data/glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189566 unique words in vocabulary\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('%s unique words in vocabulary' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our word index, search for each of our 25000 most frequents words if they exist in the pretrained GloVe embeddings and assign them to their corresponding row index in the embedding matrix. If they don't exist in the GloVe embeddings, assign a random vector :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "# Allocate the embeddings matrix\n",
    "embedding_matrix = np.zeros((max_vocab_size, embedding_dim))\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    #TOFILL\n",
    "    if i >= 25000: \n",
    "        break\n",
    "    elif word in embeddings_index:\n",
    "        embedding_matrix[i] = embeddings_index[word] \n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.normal(size=embedding_dim)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [-1.11385515e+00,  9.65080264e-01, -9.51645321e-02, ...,\n",
       "        -4.22016004e-01, -1.69327129e+00, -5.54072097e-01],\n",
       "       [-3.81940007e-02, -2.44870007e-01,  7.28120029e-01, ...,\n",
       "        -1.45899996e-01,  8.27799976e-01,  2.70619988e-01],\n",
       "       ...,\n",
       "       [ 2.97340006e-01,  2.59339988e-01, -8.60790014e-01, ...,\n",
       "         1.49480000e-01, -1.29659998e-03,  1.55450001e-01],\n",
       "       [ 1.27910003e-01, -5.11210024e-01, -7.17790008e-01, ...,\n",
       "        -3.77020001e-01,  5.42959988e-01,  5.50859988e-01],\n",
       "       [ 2.97219992e-01,  5.10179996e-02, -1.02019997e-03, ...,\n",
       "         4.93129998e-01,  2.86110014e-01,  4.56099987e-01]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now change your LSTM model so that the embedding layer is initialized with the pretrained embeddings :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                   lstm_out_dim, dropout_rate, embedding_matrix):\n",
    "    #TOFILL\n",
    "            \n",
    "    model = Sequential()\n",
    "    \n",
    "    # model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length))\n",
    "\n",
    "    model.add(Embedding(input_dim=vocab_size,\n",
    "                        output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=seq_length))\n",
    "    \n",
    "    model.add(LSTM(units=lstm_out_dim))\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    \n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    #TOKEEP\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 2730, 100)         2500000   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 2,741,001\n",
      "Trainable params: 2,741,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = get_lstm_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       lstm_out_dim, dropout_rate, embedding_matrix)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 178s 281ms/step - loss: 0.5642 - accuracy: 0.6955 - val_loss: 0.3154 - val_accuracy: 0.8682\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 176s 282ms/step - loss: 0.2524 - accuracy: 0.9014 - val_loss: 0.2524 - val_accuracy: 0.8984\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 178s 284ms/step - loss: 0.1548 - accuracy: 0.9435 - val_loss: 0.2378 - val_accuracy: 0.9034\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 178s 284ms/step - loss: 0.1037 - accuracy: 0.9673 - val_loss: 0.2691 - val_accuracy: 0.9086\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 172s 275ms/step - loss: 0.0622 - accuracy: 0.9812 - val_loss: 0.3132 - val_accuracy: 0.9058\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 90.04%\n"
     ]
    }
   ],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model_pretrained_embs2(vocab_size, embedding_dim, seq_length, \n",
    "                                   lstm_out_dim, dropout_rate, embedding_matrix):\n",
    "    #TOFILL\n",
    "            \n",
    "    model = Sequential()\n",
    "    \n",
    "    # model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length))\n",
    "\n",
    "    model.add(Embedding(input_dim=vocab_size,\n",
    "                        output_dim=embedding_dim,\n",
    "                        embeddings_initializer=Constant(embedding_matrix),\n",
    "                        input_length=seq_length))\n",
    "    \n",
    "    model.add(LSTM(units=lstm_out_dim))\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    \n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    #TOKEEP\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 2730, 100)         2500000   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 2,741,001\n",
      "Trainable params: 2,741,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model2 = get_lstm_model_pretrained_embs2(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       lstm_out_dim, dropout_rate, embedding_matrix)\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 170s 266ms/step - loss: 0.5592 - accuracy: 0.6974 - val_loss: 0.3146 - val_accuracy: 0.8726\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 165s 264ms/step - loss: 0.2558 - accuracy: 0.9017 - val_loss: 0.2587 - val_accuracy: 0.8958\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 165s 265ms/step - loss: 0.1570 - accuracy: 0.9436 - val_loss: 0.2362 - val_accuracy: 0.9108\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 170s 273ms/step - loss: 0.0975 - accuracy: 0.9680 - val_loss: 0.2832 - val_accuracy: 0.9096\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 171s 273ms/step - loss: 0.0534 - accuracy: 0.9845 - val_loss: 0.3250 - val_accuracy: 0.8890\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 90.04%\n"
     ]
    }
   ],
   "source": [
    "test_acc2 = model2.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the validation accuracy indeed progressed much faster than previously.\n",
    "\n",
    "For more speed-up, at the price of accuracy, let's fix the embeddings so that they are not trainable parameters of the model, meaning they won't be updated during training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                   lstm_out_dim, dropout_rate, embedding_matrix,\n",
    "                                    trainable_embeddings):\n",
    "    #TOFILL\n",
    "    model = Sequential()\n",
    "    \n",
    "    # model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length))\n",
    "\n",
    "    model.add(Embedding(input_dim=vocab_size,\n",
    "                        output_dim=embedding_dim,\n",
    "                        embeddings_initializer=Constant(embedding_matrix),\n",
    "                        input_length=seq_length,\n",
    "                        trainable=trainable_embeddings))\n",
    "    \n",
    "    model.add(LSTM(units=lstm_out_dim))\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    \n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    #TOKEEP\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 2730, 100)         2500000   \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 2,741,001\n",
      "Trainable params: 241,001\n",
      "Non-trainable params: 2,500,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200\n",
    "dropout_rate = 0.2\n",
    "trainable_embeddings = False\n",
    "\n",
    "model = get_lstm_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       lstm_out_dim, dropout_rate, embedding_matrix, trainable_embeddings)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the change in the number of trainable parameters in the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 138s 216ms/step - loss: 0.6173 - accuracy: 0.6417 - val_loss: 0.4665 - val_accuracy: 0.7768\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 138s 221ms/step - loss: 0.3752 - accuracy: 0.8365 - val_loss: 0.3149 - val_accuracy: 0.8688\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 134s 214ms/step - loss: 0.3093 - accuracy: 0.8691 - val_loss: 0.2911 - val_accuracy: 0.8790\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 133s 213ms/step - loss: 0.2723 - accuracy: 0.8873 - val_loss: 0.2753 - val_accuracy: 0.8860\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 133s 212ms/step - loss: 0.2513 - accuracy: 0.8984 - val_loss: 0.2576 - val_accuracy: 0.8948\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 89.26%\n"
     ]
    }
   ],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By fixing the word embeddings, the training time shrunk a bit, but the validation accuracy is progressing more slowly and reaching a limit. Depending on the network architecture, the trade-off can be interesting, here not so much, just know this is a possibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional and stacked LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs parse the text from left to right, but doing it also from right to left and concatening the two output vectors improved the results. These are called bidirectional LSTMs. It is also possible to stack multiple LSTM layers.\n",
    "\n",
    "This image is a good illustration of how these two variants work:\n",
    "\n",
    "https://www.researchgate.net/figure/Illustrations-for-basic-LSTMs-and-the-three-layer-stacked-LSTM-model-for-the-sequential_fig3_313115860\n",
    "\n",
    "\n",
    "First modify your network to make a bidirectional LSTM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                     lstm_out_dim, dropout_rate, embedding_matrix,\n",
    "                                    trainable_embeddings):\n",
    "    #TOFILL\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim=vocab_size,\n",
    "                        output_dim=embedding_dim,\n",
    "                        embeddings_initializer=Constant(embedding_matrix),\n",
    "                        input_length=seq_length))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(units=lstm_out_dim)))\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    \n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    #TOKEEP\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 2730, 100)         2500000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 400)               481600    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 2,982,001\n",
      "Trainable params: 2,982,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = get_bilstm_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       lstm_out_dim, dropout_rate, embedding_matrix, True)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 299s 471ms/step - loss: 0.5800 - accuracy: 0.6824 - val_loss: 0.3760 - val_accuracy: 0.8402\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 297s 475ms/step - loss: 0.2830 - accuracy: 0.8889 - val_loss: 0.2438 - val_accuracy: 0.9032\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 306s 490ms/step - loss: 0.1691 - accuracy: 0.9396 - val_loss: 0.2462 - val_accuracy: 0.9084\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 295s 472ms/step - loss: 0.1064 - accuracy: 0.9666 - val_loss: 0.2591 - val_accuracy: 0.9064\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 295s 473ms/step - loss: 0.0595 - accuracy: 0.9829 - val_loss: 0.3181 - val_accuracy: 0.9012\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 90.90%\n"
     ]
    }
   ],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "ns = 10\n",
    "\n",
    "for n in range(ns):\n",
    "    print(n)\n",
    "    if n == ns - 1:\n",
    "        print('yes') \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try stacking multiple bidirectional LSTM layers, where the number of layers `n_layers` is a parameter of the function building the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multilayer_bilstm_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                                lstm_out_dim, dropout_rate, embedding_matrix,\n",
    "                                                trainable_embeddings, n_layers):\n",
    "    #TOFILL\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim=vocab_size,\n",
    "                        output_dim=embedding_dim,\n",
    "                        embeddings_initializer=Constant(embedding_matrix),\n",
    "                        input_length=seq_length))\n",
    "    \n",
    "    \n",
    "    for layer in range(n_layers-1): \n",
    "        model.add(Bidirectional(LSTM(units=lstm_out_dim, return_sequences=True)))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(units=lstm_out_dim)))\n",
    "\n",
    "\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    \n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    #TOKEEP\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 2730, 100)         2500000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 2730, 200)         160800    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 2730, 200)         240800    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 3,142,601\n",
      "Trainable params: 3,142,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 100\n",
    "dropout_rate = 0.2\n",
    "n_layers = 2\n",
    "\n",
    "model = get_multilayer_bilstm_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       lstm_out_dim, dropout_rate, embedding_matrix, True, n_layers)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1250/1250 [==============================] - 812s 643ms/step - loss: 0.5457 - accuracy: 0.7121 - val_loss: 0.3063 - val_accuracy: 0.8762\n",
      "Epoch 2/5\n",
      "1250/1250 [==============================] - 805s 644ms/step - loss: 0.2851 - accuracy: 0.8856 - val_loss: 0.2789 - val_accuracy: 0.8916\n",
      "Epoch 3/5\n",
      "1250/1250 [==============================] - 801s 641ms/step - loss: 0.2069 - accuracy: 0.9235 - val_loss: 0.2545 - val_accuracy: 0.8988\n",
      "Epoch 4/5\n",
      "1250/1250 [==============================] - 800s 640ms/step - loss: 0.1627 - accuracy: 0.9442 - val_loss: 0.2929 - val_accuracy: 0.8978\n",
      "Epoch 5/5\n",
      "1250/1250 [==============================] - 793s 635ms/step - loss: 0.1474 - accuracy: 0.9496 - val_loss: 0.2674 - val_accuracy: 0.9012\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 90.14%\n"
     ]
    }
   ],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the max accuracy reached is not much better than our TF-IDF model. This happens because the full word order is actually not so important for Sentiment Analysis. For this task, Convolutional Neural Networks can attain comparable performances faster, as they have simpler architectures. But that's not true for other task such as translation, question answering, ... (which are tasks that are a bit too long to train to be included in this course, hence the choice of sentiment an analysis to practice RNNs).\n",
    "\n",
    "Let's do it with a convolutional model by using 1D convolution with a kernel size of 3 over the word embeddings (this means that it will convolve the embeddings of the consecutive words 3 by 3), followed by a 1D max pooling and a dense ReLU layer before the final sigmoid :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                                out_dim, dropout_rate, embedding_matrix,\n",
    "                                                trainable_embeddings):\n",
    "    #TOFILL\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim=vocab_size,\n",
    "                        output_dim=embedding_dim,\n",
    "                        embeddings_initializer=Constant(embedding_matrix),\n",
    "                        input_length=seq_length))\n",
    "    \n",
    "    model.add(Conv1D(kernel_size=3, filters=200, padding = \"same\"))\n",
    "\n",
    "    model.add(MaxPool1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    #TOKEEP\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 2730, 100)         2500000   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 2730, 200)         60200     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1365, 200)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 273000)            0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                8736032   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 11,296,265\n",
      "Trainable params: 11,296,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "out_dim = 200\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = get_conv_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       out_dim, dropout_rate, embedding_matrix, True)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 64s 102ms/step - loss: 0.5652 - accuracy: 0.7207 - val_loss: 0.3946 - val_accuracy: 0.8262\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 63s 101ms/step - loss: 0.2485 - accuracy: 0.9021 - val_loss: 0.2660 - val_accuracy: 0.8928\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 64s 102ms/step - loss: 0.1458 - accuracy: 0.9444 - val_loss: 0.2993 - val_accuracy: 0.8944\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 64s 102ms/step - loss: 0.0807 - accuracy: 0.9704 - val_loss: 0.4071 - val_accuracy: 0.8790\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 64s 102ms/step - loss: 0.0489 - accuracy: 0.9818 - val_loss: 0.4520 - val_accuracy: 0.8896\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 89.04%\n"
     ]
    }
   ],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very optional parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parts are meant to be resources to explore if you are interested in the advanced concept of attention in deep nets. There are explanation links, as well as links with code for each of them, but don't feel obliged to implement all of them, these are meant to help understanding each of these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention is a mechanism that changes the output of an LSTM : instead of outputting the final hidden state vector $h_n$ where $n$ is the length of the encoded text, attention plugs on top of a LSTM and returns a combination of all the hidden state vectors at each word position $\\ \\sum_{t=1}^n \\alpha_t h_t$ (where $\\alpha_t \\in (0,1))$, and thus allows to pay a different attention to each part of the text, hence the name. \n",
    "\n",
    "It has been originally proposed for sequence to sequence models, like translation models, where there is a different attention combination computed for each translated output word. It is thus less useful for text classification, but it can be adapted, by computing a single output combination of all the hidden states, as explained in Section 3.3 of the following article :\n",
    "\n",
    "https://www.aclweb.org/anthology/P16-2034.pdf\n",
    "\n",
    "Here is a link about how to apply attention for text classification with Keras:\n",
    "\n",
    "https://www.kaggle.com/yshubham/simple-lstm-for-text-classification-with-attention\n",
    "\n",
    "\n",
    "You can also read the following link to understand how attention works in sequence to sequence models, which are nothing more than a reversed LSTM (the decoder) on top of a first LSTM (the encoder), in this case for translation where it helps aligning words in two different languages :\n",
    "\n",
    "https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer architecture for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State of the art models in NLP are not RNNs anymore, but Transformers. Transformers do not read text sequentially like RNNs, the core concept of Transformers is self-attention, an attention mechanism that combine separately each word embedding with the other word embeddings of the text. There are multiple such attention mechanisms called \"attention heads\" in a layer, and multiple such layers are stacked.\n",
    "\n",
    "Read this article to understand the self-attention layer:\n",
    "\n",
    "https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a\n",
    "\n",
    "This article explains very well the Transformer for sequence to sequence models (again remember that a text classification model is just the encoder part of a sequence to sequence model) :\n",
    "\n",
    "http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "Keras code to do text classification with a Transformer :\n",
    "\n",
    "https://keras.io/examples/nlp/text_classification_with_transformer/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current state-of-the-art performance for text classification are achieved by doing transfer learning from the BERT model. The BERT model combines different techniques including the Transformer to pretrain in an unsupervised fashion on plain text. The last layers of BERT provide a high-level contextual representation of english sentences, and can then be reused in any NLP deep model.\n",
    "\n",
    "The BERT model : http://jalammar.github.io/illustrated-bert/\n",
    "\n",
    "Keras application for text classification : https://www.section.io/engineering-education/classification-model-using-bert-and-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ML_4_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f2484187ee42d3d5e5c171359c7022c6d84d0332ce92b6363664a574451e230"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
